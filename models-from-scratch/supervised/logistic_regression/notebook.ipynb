{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "The goal of a binary logistic regression is to model the probability of a random variable $Y$ being 0 or 1 given experimental data. It may be trained on multiclass problems as well, but the focus will be on binary classes.\n",
    "\n",
    "The logistic regression curve is defined by the sigmoid function, with equation presented below.\n",
    "\n",
    "<center>\n",
    "    $ \\Pr(Y=1\\mid X; \\hat{\\beta}) = {\\dfrac {1}{1+e^{-X^{T}\\hat{\\beta} + \\beta_0}}} = h_\\hat{\\beta}(X) \\implies \\Pr(Y=0\\mid X; \\hat{\\beta}) = 1 - h_\\hat{\\beta}(X) $\n",
    "</center>\n",
    "\n",
    "Where $\\hat{\\beta}$ is the vector of coefficients, $\\beta_0$ is the bias and $X$ is the vector of features.\n",
    "\n",
    "Let's start by defining them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(weights, features, bias):\n",
    "    y_hat = sigmoid(np.matmul(weights, features) + bias)\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For logistic regression, least squares estimation is not capable of producing minimum variance unbiased estimators for the actual parameters. In its place, maximum likelihood estimation / cross-entropy minimization is used to solve for the parameters that best fit the data.\n",
    "\n",
    "The cross-entropy is defined by\n",
    "\n",
    "<center>\n",
    "    $ H(y, p) = - \\sum\\limits_{1}^n y_i\\ln(p_i)+(1-y_i)\\ln(1-p_i) = Error$\n",
    "</center>\n",
    "\n",
    "Where $y$ is the binary classification distribution and $p$ the predicted event probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y, p):\n",
    "    y = np.array(y)\n",
    "    p = np.array(p)\n",
    "    ce = -y * np.log(p) - (1 - y) * np.log(1 - p)\n",
    "    return ce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimize the loss function, gradient descent may be used, it works by updating the weights and bias proportionally to the derivative of the loss function. The cross entropy derivative is defined by\n",
    "\n",
    "<center>\n",
    "    $ \\nabla Error =  -(y - \\hat{y})(x_1, x_2, \\dots, x_n, 1) $\n",
    "</center>\n",
    "\n",
    "Therefore, the weights and bias are updated in the following way:\n",
    "\n",
    "<center>\n",
    "    $ w_i^{\\prime} \\leftarrow w_i + \\alpha (y - \\hat{y}) x_i $\n",
    "</center>\n",
    "\n",
    "<center>\n",
    "    $b^{\\prime} \\leftarrow b + \\alpha (y - \\hat{y})$\n",
    "</center>\n",
    "\n",
    "Where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(x, y, y_hat, weights, bias, learning_rate):\n",
    "    \n",
    "    bias += learning_rate * (y - y_hat)\n",
    "    weights += learning_rate * (y - y_hat) * x\n",
    "\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! We are ready to train a logistic regression. For simplicity batch gradient descent will be used, but for larger datasets stochastic gradient descent presents a faster convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    # WIP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
